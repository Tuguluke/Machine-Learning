{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6988343e",
   "metadata": {},
   "source": [
    "# CSCI 5622: Machine Learning\n",
    "## Fall 2023\n",
    "### Instructor: Daniel Acuna, Associate Professor, Department of Computer Science, University of Colorado at Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fdf184",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e51cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Luk Letif\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1b70e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a996eb-9dfe-40be-bbd8-6eae4cb4e0db",
   "metadata": {},
   "source": [
    "# Homework 6 - Topic Modeling (50 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1872a5f-23fc-42c6-865a-2b7f1111b7c7",
   "metadata": {},
   "source": [
    "## Question 1: (10 pts) Dataset Acquisition and Preprocessing\n",
    "\n",
    "**Objective:** In this question, you will acquire a dataset of text and perform preprocessing steps to prepare it for topic modeling using scikit-learn. This preprocessing step is crucial for effective topic modeling using algorithms like NMF (Non-negative Matrix Factorization) and LDA (Latent Dirichlet Allocation).\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Dataset Acquisition:** \n",
    "   - Download the '20 Newsgroups' dataset, a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups.\n",
    "   - URL for the dataset: `http://qwone.com/~jason/20Newsgroups/20news-19997.tar.gz`\n",
    "   - Use the `fetch_20newsgroups` function from `sklearn.datasets` to load the dataset. \n",
    "   - Focus on a subset of 4 newsgroups for simplicity: `['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']`.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   - Tokenize and extract features from the text data using `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
    "   - Perform the following preprocessing steps:\n",
    "     - Convert all text to lowercase.\n",
    "     - Remove stopwords.\n",
    "     - Use a `max_df` of 0.95 and `min_df` of 2.\n",
    "     - Extract the top 1000 most frequent words.\n",
    "\n",
    "Use the test cell to guide you as to which variables to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ef8df0-4a2c-4318-a96d-5671d13fcc45",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df4f447d7f9e40a18da7495fe6901472",
     "grade": false,
     "grade_id": "cell-d2f81db86b4b04f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# Focus on a subset of 4 newsgroups for simplicity: ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space'].\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "# Use the fetch_20newsgroups function from sklearn.datasets to load the dataset.\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "# Perform the preprocessing steps\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "processed_features = tfidf.fit_transform(newsgroups_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778d6a92-07ea-4fed-88ea-b1f952bbafd1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25f0ddee46fbd1e05ec7bb888435a768",
     "grade": true,
     "grade_id": "cell-f101fd787a16e591",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 10 pts\n",
    "def test_dataset_download():\n",
    "    global newsgroups_data\n",
    "    \n",
    "    assert 'newsgroups_data' in globals(), \"Dataset not loaded with the variable name 'newsgroups_data'\"\n",
    "    assert len(newsgroups_data.data) > 0, \"Dataset seems to be empty\"\n",
    "\n",
    "def test_preprocessing():\n",
    "    global tfidf, processed_features\n",
    "    \n",
    "    assert 'tfidf' in globals(), \"TfidfVectorizer not defined\"\n",
    "    assert hasattr(tfidf, 'fit_transform'), \"TfidfVectorizer not properly initialized\"\n",
    "    assert tfidf.get_feature_names_out().shape[0] == 1000, \"The number of features extracted does not match 1000\"\n",
    "\n",
    "# Run the tests\n",
    "test_dataset_download()\n",
    "test_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad5636-6d89-44f1-9411-118f5eb4ba8a",
   "metadata": {},
   "source": [
    "## Question 2: Non-negative Matrix Factorization (NMF) and Performance Validation\n",
    "\n",
    "**Objective:** Apply NMF to the preprocessed text dataset with different numbers of components (topics) and evaluate the performance using a specific metric. This exercise will help you understand the impact of choosing different dimensions (number of topics) in topic modeling.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Implement NMF:**\n",
    "   - Apply NMF on the preprocessed text data (from Question 1) using scikit-learn's `NMF` class.\n",
    "   - Experiment with different numbers of components (use 5, 10, 15, 20).\n",
    "   - Use the 'frobenius' norm as the loss function and a random state of 42 for reproducibility.\n",
    "\n",
    "2. **Performance Validation:**\n",
    "   - Evaluate the performance of each NMF model using the Frobenius norm of the matrix difference (i.e., the difference between the original data matrix and the reconstructed matrix from the NMF components and coefficients).\n",
    "   - Store the Frobenius norm values for each number of components in a list or dictionary for comparison.\n",
    "\n",
    "Look at the test to determine where you have to save the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdab92ea-1d3c-4cc3-89d7-97d8f756b7c3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "495755304501ea4d81e1f7b3b2a4ad64",
     "grade": false,
     "grade_id": "cell-922189026b8a5a2e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# same as above\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words='english', max_df=0.95, min_df=2, max_features=1000)\n",
    "processed_features = tfidf.fit_transform(newsgroups_data.data)\n",
    "\n",
    "# init names\n",
    "nmf_models = {}\n",
    "frobenius_norms = {}\n",
    "#Experiment with different numbers of components (use 5, 10, 15, 20).\n",
    "n_components_list = [5, 10, 15, 20]\n",
    "\n",
    "# Implement NMF\n",
    "for n_components in n_components_list:\n",
    "    # random state of 42 for reproducibility.\n",
    "    nmf = NMF(n_components=n_components, random_state=42, solver='cd', beta_loss='frobenius',max_iter=500) # Maximum number of iterations 200 reached\n",
    "    W = nmf.fit_transform(processed_features)\n",
    "    H = nmf.components_\n",
    "    # TODO: err\n",
    "    reconstructed = np.dot(W, H)\n",
    "    # Use the 'frobenius' norm as the loss function and a \n",
    "    frobenius_norm = np.linalg.norm(processed_features - reconstructed, 'fro')\n",
    "\n",
    "    # Store the Frobenius norm values for each number of components in a list or dictionary for comparison.\n",
    "    nmf_models[n_components] = nmf\n",
    "    frobenius_norms[n_components] = frobenius_norm\n",
    "    \n",
    "# TODO:test_optimal_components_selection\n",
    "optimal_components = min(frobenius_norms, key=frobenius_norms.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0672f77-a503-458e-805b-7191aa118a89",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b08d1b1aa677a8fa8f8bfbb67088201",
     "grade": true,
     "grade_id": "cell-b3b0e2af1b7489c4",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 15 pts\n",
    "def test_nmf_models():\n",
    "    assert 'nmf_models' in globals(), \"nmf_models dictionary not defined\"\n",
    "    assert isinstance(nmf_models, dict), \"nmf_models should be a dictionary\"\n",
    "    assert all(isinstance(nmf, NMF) for nmf in nmf_models.values()), \"All values in nmf_models should be instances of NMF\"\n",
    "    assert all(n_components in nmf_models for n_components in [5, 10, 15, 20]), \"NMF models for all specified component numbers should be created\"\n",
    "\n",
    "def test_frobenius_norms():\n",
    "    assert 'frobenius_norms' in globals(), \"frobenius_norms dictionary not defined\"\n",
    "    assert isinstance(frobenius_norms, dict), \"frobenius_norms should be a dictionary\"\n",
    "    assert len(frobenius_norms) == 4, \"There should be four Frobenius norm values for the four NMF models\"\n",
    "    assert all(isinstance(norm, float) for norm in frobenius_norms.values()), \"All values in frobenius_norms should be floats\"\n",
    "\n",
    "def test_optimal_components_selection():\n",
    "    assert 'optimal_components' in globals(), \"Variable 'optimal_components' not defined\"\n",
    "    assert optimal_components in [5, 10, 15, 20], \"Optimal components not selected from the predefined list\"\n",
    "\n",
    "# Run the tests\n",
    "test_nmf_models()\n",
    "test_frobenius_norms()\n",
    "test_optimal_components_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac6cb15-8c43-48f9-a7b4-fe6611193d77",
   "metadata": {},
   "source": [
    "## Question 3: Latent Dirichlet Allocation (LDA) and Topic Interpretation\n",
    "\n",
    "**Objective:** Apply LDA to the preprocessed text dataset from Question 1, extract 5 topics, and interpret what each topic represents. Use `CountVectorizer` instead of TF-IDF.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. **Implement LDA:**\n",
    "   - Apply LDA on the preprocessed text data using scikit-learn's `LatentDirichletAllocation` class.\n",
    "   - Extract exactly 5 topics from the dataset.\n",
    "\n",
    "2. **Print and Interpret Topics:**\n",
    "   - For each topic, print the top 10 words based on their importance in the topic.\n",
    "   - Write a brief interpretation for each topic, discussing what you think the topic represents based on the top words.\n",
    "\n",
    "Use the test cell you guide about the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702529af-2be0-4763-9074-b8d40bbcf1f3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d086eda0542476a32d4075999bc272a",
     "grade": false,
     "grade_id": "cell-593413c409e54083",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for Topic 1: ['jpeg', 'image', 'file', 'gif', 'images', 'color', 'bit', 'format', 'files', 'use']\n",
      "Hypothetical interpretation of Topic 1: Likely about space exploration (words like 'nasa', 'space', 'orbit')\n",
      "\n",
      "Top 10 words for Topic 2: ['space', 'nasa', 'earth', 'launch', 'orbit', 'shuttle', 'moon', 'time', 'mission', 'solar']\n",
      "Hypothetical interpretation of Topic 2: Computer technology (words like 'software', 'graphics', 'image')\n",
      "\n",
      "Top 10 words for Topic 3: ['edu', 'graphics', 'data', 'image', 'ftp', 'available', 'pub', 'software', 'mail', 'information']\n",
      "Hypothetical interpretation of Topic 3: Religion and philosophy (words like 'god', 'morality', 'belief')\n",
      "\n",
      "Top 10 words for Topic 4: ['god', 'jesus', 'people', 'bible', 'know', 'said', 'did', 'like', 'just', 'say']\n",
      "Hypothetical interpretation of Topic 4: Online communities and discussion (words like 'internet', 'email', 'group')\n",
      "\n",
      "Top 10 words for Topic 5: ['don', 'people', 'think', 'just', 'does', 'god', 'like', 'say', 'know', 'way']\n",
      "Hypothetical interpretation of Topic 5: Science and research (words like 'data', 'study', 'theory')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 20 pts\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "# same as above\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "# Use CountVectorizer instead of TF-IDF.\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words='english')\n",
    "count_data = count_vectorizer.fit_transform(newsgroups_data.data)\n",
    "\n",
    "# Apply LDA on the preprocessed text data using scikit-learn's LatentDirichletAllocation class.\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=0)\n",
    "lda_model.fit(count_data)\n",
    "# For each topic, print the top 10 words based on their importance in the topic.\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "topics_words = []\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_feature_indices = topic.argsort()[-10:][::-1]  # conc\n",
    "    topic_words = [feature_names[i] for i in top_feature_indices]\n",
    "    topics_words.append(topic_words)\n",
    "    \n",
    "# Hypothetical interpretations:\n",
    "# Topic 1: Likely about space exploration (words like 'nasa', 'space', 'orbit')\n",
    "# Topic 2: Computer technology (words like 'software', 'graphics', 'image')\n",
    "# Topic 3: Religion and philosophy (words like 'god', 'morality', 'belief')\n",
    "# Topic 4: Online communities and discussion (words like 'internet', 'email', 'group')\n",
    "# Topic 5: Science and research (words like 'data', 'study', 'theory')\n",
    "interpretations = [\n",
    "    \"Likely about space exploration (words like 'nasa', 'space', 'orbit')\",\n",
    "    \"Computer technology (words like 'software', 'graphics', 'image')\",\n",
    "    \"Religion and philosophy (words like 'god', 'morality', 'belief')\",\n",
    "    \"Online communities and discussion (words like 'internet', 'email', 'group')\",\n",
    "    \"Science and research (words like 'data', 'study', 'theory')\"\n",
    "]\n",
    "#Print and Interpret Topics:\n",
    "for i, (topic_words, interpretation) in enumerate(zip(topics_words, interpretations), 1):\n",
    "    print(f\"Top 10 words for Topic {i}: {topic_words}\")\n",
    "    print(f\"Hypothetical interpretation of Topic {i}: {interpretation}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a35f879-1a38-4c70-9b9f-fc999d0f36e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c38f970bc1c97621f7c37d8796bffdc",
     "grade": true,
     "grade_id": "cell-42928f50f2892932",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# 15 pts\n",
    "def test_lda_model():\n",
    "    assert 'lda_model' in globals(), \"LDA model not defined\"\n",
    "    assert isinstance(lda_model, LatentDirichletAllocation), \"lda_model is not an instance of LatentDirichletAllocation\"\n",
    "    assert lda_model.n_components == 5, \"LDA model should have exactly 5 topics\"\n",
    "\n",
    "def test_topic_words():\n",
    "    assert 'topics_words' in globals(), \"topics_words not defined\"\n",
    "    assert isinstance(topics_words, list), \"topics_words should be a list\"\n",
    "    assert all(isinstance(topic, list) for topic in topics_words), \"Each topic in topics_words should be a list\"\n",
    "    assert all(len(topic) == 10 for topic in topics_words), \"Each topic should contain exactly 10 words\"\n",
    "\n",
    "# Run the tests\n",
    "test_lda_model()\n",
    "test_topic_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615d948-53ca-4ace-9992-bdeb453717d8",
   "metadata": {},
   "source": [
    "**Q. 3.2** (5 pts) What do each of the topics represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631dd00-be2b-431b-b7fa-310fffa2af40",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1afaf16b215ccf822fd42de3982b730",
     "grade": true,
     "grade_id": "cell-04be3755268a6783",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Topic 1: Computer technology\n",
    "Topic 2: space exploration\n",
    "Topic 3: Science and research\n",
    "Topic 4: Religion and philosophy \n",
    "Topic 5: Religion and philosophy + Online communities and discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096a9d57-ace1-4ec2-9acb-8c13a3165822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
